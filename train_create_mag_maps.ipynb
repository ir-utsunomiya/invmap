{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "from __future__ import print_function\n",
    "\n",
    "import rospy\n",
    "import rosbag\n",
    "import itertools\n",
    "from  tqdm import tqdm\n",
    "from geometry_msgs.msg import PoseWithCovarianceStamped\n",
    "import tf\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import GPy\n",
    "import time\n",
    "from sklearn.cluster import KMeans\n",
    "import cPickle as pickle\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Mag Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = '/mnt/matrix/rosbag/processed_data/nict'\n",
    "all_maps = 'udai-01'\n",
    "save_model = False\n",
    "thr_d = 6\n",
    "\n",
    "\n",
    "# Load data\n",
    "print('\\n####################\\n  map: {:s} \\n####################'.format(map_name))\n",
    "\n",
    "save_filename = '{0:s}/{1:s}/invisible-maps/{1:s}_MagzData.npz'.format(root_folder,map_name)\n",
    "data = np.load(save_filename,allow_pickle=True)\n",
    "print(data['XYZ'].shape,data['MZ'].shape)\n",
    "#plt.plot(data['MZ'])\n",
    "\n",
    "## Data split\n",
    "### Too much data to handle using a single GPy\n",
    "\n",
    "# Save model\n",
    "save_filename_ = '{0:s}/{1:s}/invisible-maps/{1:s}_MagzModel_'.format(root_folder,map_name)\n",
    "\n",
    "#create n clusters\n",
    "print('>> Learning clustering')\n",
    "n_clusters = 6\n",
    "kmeans = KMeans(n_clusters=n_clusters).fit(data['XYZ'][:,0:2])\n",
    "\n",
    "if save_model:\n",
    "    # save kmeans model\n",
    "    save_filename = '{:s}kmeans.pkl'.format(save_filename_)\n",
    "    pickle.dump(kmeans, open(save_filename, \"wb\"))\n",
    "\n",
    "c_index = kmeans.predict(data['XYZ'][:,0:2])\n",
    "print('Cluster centers          npoints')\n",
    "for i in range(n_clusters):\n",
    "    print('[{: 9.2f} {: 9.2f}] {: 8d}'.format(kmeans.cluster_centers_[i,0],\n",
    "                                              kmeans.cluster_centers_[i,1],\n",
    "                                              int(np.sum(c_index==i))))\n",
    "print('')\n",
    "#f, ax = plt.subplots(figsize=(16,9))\n",
    "#for i in range(n_clusters):\n",
    "#    index = c_index==i\n",
    "#    ax.scatter(data['XYZ'][index,0],\n",
    "#               data['XYZ'][index,1])\n",
    "\n",
    "print('>> Learning a GPy model per cluster')\n",
    "for i in range(n_clusters):\n",
    "    # for points in the cluster\n",
    "    index = c_index==i\n",
    "    datax = data['XYZ'][index,:]\n",
    "    datay = data['MZ'][index,:]\n",
    "\n",
    "    #f, ax = plt.subplots(figsize=(16,9))\n",
    "    #ax.scatter(data['XYZ'][:,0],data['XYZ'][:,1],c='gray')\n",
    "    #ax.scatter(datax[:,0],datax[:,1],c='r')\n",
    "\n",
    "    # for points not in the cluster (check if thr_d meters from the cluster)\n",
    "    nindex = c_index!=i\n",
    "    ndatax = data['XYZ'][nindex,:]\n",
    "    ndatay = data['MZ'][nindex,:]\n",
    "    datax_extra = list()\n",
    "    datay_extra = list()\n",
    "    for nx,ny in zip(ndatax,ndatay):\n",
    "        if (np.min(np.linalg.norm(nx-datax,axis=1))) < thr_d:\n",
    "            datax_extra.append(nx)\n",
    "            datay_extra.append(ny)\n",
    "    datax = np.append(datax,np.asarray(datax_extra),axis=0)\n",
    "    datay = np.append(datay,np.asarray(datay_extra),axis=0)\n",
    "\n",
    "    print('\\n{: 2d} | data points {: 9d}\\n'.format(i,datax.shape[0]))\n",
    "\n",
    "    # train model\n",
    "    print('  >> Creating model')\n",
    "    #kernel = GPy.kern.Matern52(3)\n",
    "    kernel = GPy.kern.RBF(3)\n",
    "    #kernel.lengthscale[0] = 1.5\n",
    "    kernel.lengthscale[0] = 1.5\n",
    "    kernel.lengthscale.fix()\n",
    "    ##kernel.lengthscale.set_prior(GPy.priors.InverseGamma(1.5,1.5))\n",
    "    model = GPy.models.GPRegression(datax,datay,kernel=kernel)\n",
    "\n",
    "    print('  >> Optimizing model')\n",
    "    t0 = time.time()\n",
    "    model.optimize()\n",
    "    print(model)\n",
    "\n",
    "    print('\\n  >> Elapsed min: {:.2f}'.format((time.time()-t0)/60.))\n",
    "\n",
    "    if save_model:\n",
    "        save_filename = '{:s}{:02d}.npz'.format(save_filename_,i)\n",
    "        print('  >> Saving model {:s}'.format(save_filename))\n",
    "        np.savez_compressed(save_filename,param_array=model.param_array,datax=datax,datay=datay)\n",
    "\n",
    "    del datax, datax_extra, datay, datay_extra, ndatax, ndatay, kernel, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Mag Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 0.3\n",
    "res = 0.1\n",
    "\n",
    "print('\\n####################\\n  map: {:s} \\n####################'.format(map_name))\n",
    "\n",
    "# Load data\n",
    "save_filename = '{0:s}/{1:s}/invisible-maps/{1:s}_MagzData.npz'.format(root_folder,map_name)\n",
    "data = np.load(save_filename,allow_pickle=True)\n",
    "print(data['XYZ'].shape,data['MZ'].shape)\n",
    "#plt.plot(data['MZ'])\n",
    "\n",
    "# create map array\n",
    "xmin,xmax = np.min(data['XYZ'][:,0]),np.max(data['XYZ'][:,0])\n",
    "ymin,ymax = np.min(data['XYZ'][:,1]),np.max(data['XYZ'][:,1])\n",
    "x = np.arange(np.floor(xmin-20),np.ceil(xmax+20)+0.01,step=res)\n",
    "y = np.arange(np.floor(ymin-20),np.ceil(ymax+20)+0.01,step=res)\n",
    "print('>> X,Y data: ',x.shape, y.shape)\n",
    "xy = np.meshgrid(x,y)\n",
    "\n",
    "XYtest = np.concatenate([xy[0].reshape(-1,1),\n",
    "                         xy[1].reshape(-1,1),\n",
    "                         height*np.ones_like(xy[0].reshape(-1,1))],axis=1).astype(np.float32)\n",
    "\n",
    "save_filename_ = '{0:s}/{1:s}/invisible-maps/{1:s}_MagzModel_'.format(root_folder,map_name)\n",
    "\n",
    "# load kmeans\n",
    "print('>> Loading kmeans')\n",
    "save_filename = '{:s}kmeans.pkl'.format(save_filename_)\n",
    "kmeans = pickle.load(open(save_filename, \"rb\"))\n",
    "## index of XYtest points better predicted by each model\n",
    "print('>> Computing XY index')\n",
    "c_index = kmeans.predict(XYtest[:,0:2])\n",
    "\n",
    "Predictions_mean = np.zeros((XYtest.shape[0],data['MZ'].shape[1]))\n",
    "Predictions_var = 100*np.ones((XYtest.shape[0],1))\n",
    "\n",
    "print('>> Computing predictions per model\\n'); time.sleep(.2);\n",
    "\n",
    "for model_i in range(kmeans.n_clusters):\n",
    "    # load model\n",
    "    save_filename = '{:s}{:02d}.npz'.format(save_filename_,model_i)\n",
    "    model_data = np.load(save_filename)\n",
    "    #kernel = GPy.kern.Matern52(3)\n",
    "    kernel = GPy.kern.RBF(3)\n",
    "    #kernel.lengthscale.set_prior(GPy.priors.InverseGamma(1.5,1.5))\n",
    "    model = GPy.models.GPRegression(model_data['datax'],model_data['datay'],kernel=kernel)\n",
    "    model[:] = model_data['param_array']\n",
    "    model.initialize_parameter()\n",
    "\n",
    "    XYindex = list() \n",
    "    #for i,ci in enumerate(c_index):\n",
    "    #    if ci==model_i: XYindex.append(i)\n",
    "    ci = c_index==model_i\n",
    "    XYi = XYtest[ci,:]\n",
    "\n",
    "    predictions_mean_i = np.zeros((XYi.shape[0],data['MZ'].shape[1]))\n",
    "    predictions_var_i = 100*np.ones((XYi.shape[0],1))\n",
    "\n",
    "    # compute predictions in batches\n",
    "    batch_eval = 10000\n",
    "    iterations = int(np.ceil(XYi.shape[0]/batch_eval))\n",
    "\n",
    "    # predictions\n",
    "    for i in tqdm(range(iterations+1),desc='Model {:02d}/{:02d}'.format(model_i+1,kmeans.n_clusters)):\n",
    "        try:\n",
    "            xy = XYi[i*batch_eval:(i+1)*batch_eval,:]\n",
    "        except:\n",
    "            xy = XYi[i*batch_eval:-1,:]\n",
    "        ipoints = xy.shape[0]\n",
    "        predictions_mean,predictions_variance = model.predict(xy)\n",
    "        #predictions_mean = model_i*np.ones_like(xy[:,0]).reshape(-1,1)\n",
    "        #predictions_variance = np.ones_like(xy[:,0]).reshape(-1,1)\n",
    "        predictions_mean_i[i*batch_eval:i*batch_eval+ipoints,:] = predictions_mean+data['mz_offset']\n",
    "        predictions_var_i[i*batch_eval:i*batch_eval+ipoints,:]  = predictions_variance\n",
    "\n",
    "    Predictions_mean[ci] = predictions_mean_i\n",
    "    Predictions_var[ci] = predictions_var_i\n",
    "\n",
    "save_filename = '{0:s}/{1:s}/invisible-maps/{1:s}_MagzMap_{2:03d}.npz'.format(root_folder,map_name,int(height*100))\n",
    "print('  >> Saving model {:s}'.format(save_filename))\n",
    "np.savez_compressed(save_filename,XY=XYtest,Mag=Predictions_mean,Mag_var=Predictions_var)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
